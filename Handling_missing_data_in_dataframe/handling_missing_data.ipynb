{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c539261e-3bec-44d6-8f63-ee4f297f8699",
   "metadata": {},
   "source": [
    "Often data sources are incomplete, which means you will have missing data, you have 3 basic options for filling in missing data (you will personally have to make the decision for what is the right approach:\n",
    "\n",
    "\n",
    "#### Just keep the missing data points.\n",
    "\n",
    "#### Drop them missing data points (including the entire row)\n",
    "\n",
    "#### Fill them in with some other value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7de8a3-9f3d-4516-8ee2-8cbad593e0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/19 22:37:40 WARN Utils: Your hostname, dhiraj resolves to a loopback address: 127.0.1.1; using 192.168.10.66 instead (on interface wlo1)\n",
      "24/03/19 22:37:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/19 22:37:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# creating sparksession object\n",
    "spark = SparkSession.builder.appName(\"handling_missing_data\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360fb0a9-e36d-4e0b-92d6-cc082b6e8f80",
   "metadata": {},
   "source": [
    "#### Reading/Loading .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d350566-5c0c-470f-82c9-4ae1014c2d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"../data/csv/containsnull.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0486789a-ac79-4454-842f-3ee04a820bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70219b10-3963-47b1-b44f-14c0771a18ef",
   "metadata": {},
   "source": [
    "### `Drop` the `missing` data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d025be-66f2-430b-bbdb-c7943a01b71e",
   "metadata": {},
   "source": [
    "You can use the .na functions for missing data. \n",
    "\n",
    "The drop command has the following parameters:\n",
    "\n",
    "df.na.drop(how='any', thresh=None, subset=None)\n",
    "\n",
    "\n",
    "* param how: 'any' or 'all'.\n",
    "\n",
    "    If 'any', drop a row if it contains any nulls.\n",
    "    If 'all', drop a row only if all its values are null.\n",
    "\n",
    "\n",
    "* param thresh: int, default None\n",
    "\n",
    "    If specified, drop rows that have less than `thresh` non-null values.\n",
    "    This overwrites the `how` parameter.\n",
    "\n",
    "* param subset: \n",
    "    optional list of column names to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c790f676-f663-47c9-87cd-25bcfbc82e61",
   "metadata": {},
   "source": [
    "#### Dropping `any row` that contains missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a08fe7a3-80dc-41fb-9a06-816940db252d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417d6af-1cf5-4116-b2d0-facc631374eb",
   "metadata": {},
   "source": [
    "#### Dropping the row, `if row has not even 2 non-null values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf808150-6fc7-4a7b-b759-f412c5496f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ce9ff8-623e-416a-8fc2-71c91e1a4db2",
   "metadata": {},
   "source": [
    "#### Dropping row, `if row has any null values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd8b10a7-967c-491d-85b7-bb82d989a95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6a38d-1ecc-465a-aae1-fc677097c266",
   "metadata": {},
   "source": [
    "#### Dropping row, `if row has all null values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "382951be-ea9d-4d39-8e96-5b691926efa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14761c62-833b-4457-ab70-0fa3d171d46e",
   "metadata": {},
   "source": [
    "### `Fill` the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3da257e-4a0b-4a0f-a1c1-27db6ea1b193",
   "metadata": {},
   "source": [
    "We can also fill the missing values with new values. \n",
    "\n",
    "If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. \n",
    "\n",
    "Watch below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc628945-872f-410d-96fb-8a741cda0323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+\n",
      "|  Id|     Name|Sales|\n",
      "+----+---------+-----+\n",
      "|emp1|     John| NULL|\n",
      "|emp2|NEW VALUE| NULL|\n",
      "|emp3|NEW VALUE|345.0|\n",
      "|emp4|    Cindy|456.0|\n",
      "+----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('NEW VALUE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51cf4a81-1cfa-4a6b-ae25-6e63ceb3f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| NULL|  0.0|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb7e0a-efe9-4c6f-8a97-a3bd8cd59f4b",
   "metadata": {},
   "source": [
    "#### BEST WAY:  Usually you should specify what columns you want to fill with the subset parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1131c81-b65f-4f05-bc8b-30734d447998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| NULL|\n",
      "|emp2|No Name| NULL|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('No Name',subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b04d200f-6e53-44ee-b64a-5dccb26842ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| NULL|  0.0|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0, subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61373882-dd3b-42fc-bb2c-76f85a9befe8",
   "metadata": {},
   "source": [
    "#### Best practice is to fill `null/nan` values with the `mean value` for the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22a2fc14-c51e-41f6-b41b-13d10743409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "mean_val = df.select(mean(df['Sales'])).collect()\n",
    "\n",
    "## Weird nested formatting of Row object\n",
    "# mean_val[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebe1422b-6a58-4d27-84b3-bb2cc4e82d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sales = mean_val[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e823dc39-03ff-44fb-82f2-048d09cfc7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| NULL|400.5|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filling null values with `mean` values\n",
    "\n",
    "df.na.fill(mean_sales, [\"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b93bb3-149b-49e5-92a8-958fe8db6958",
   "metadata": {},
   "source": [
    "### One linear code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24ade671-4dd0-47b6-9cfa-428a63db0e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| NULL|400.5|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(df.select(mean(df['Sales'])).collect()[0][0],['Sales']).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
